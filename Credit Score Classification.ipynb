{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_original = pd.read_csv('train.csv')\n",
    "df_train = df_train_original.copy()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preview Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check Data Size\n",
    "print('Train Data Size : ',df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check Train dataset\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Observations : </h4>\n",
    "    \n",
    "1. There are missing values present in dataset.\n",
    "2. Train dataset has both numerical and string values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check statistical values for fields with numerical datatype\n",
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check statistical values for fields with other than numerical datatype\n",
    "df_train.describe(exclude=np.number).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Customer_ID has 12500 unique values. It means we have data of 12500 customers.\n",
    "2. Month has only 8 unique values. Better to analyse further which months are present. \n",
    "3. Age has 1788 unique values. This looks strange as general age range is from 0-100. \n",
    "4. SSN has 12501 unique values, whereas Customer_ID only has only 12500 unique values. There is a possibility that incorrect SSN value is entered for one of the customer as same person can't have multiple SSN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "Created following functions that will help in exploring,analysing & cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_column_details(df,column):\n",
    "    print(\"Details of\",column,\"column\")\n",
    "    \n",
    "    #DataType of column\n",
    "    print(\"\\nDataType: \",df[column].dtype)\n",
    "    \n",
    "    #Check if null values are present\n",
    "    count_null = df[column].isnull().sum()\n",
    "    if count_null==0:\n",
    "        print(\"\\nThere are no null values\")\n",
    "    elif count_null>0:\n",
    "        print(\"\\nThere are \",count_null,\" null values\")\n",
    "        \n",
    "    #Get Number of Unique Values\n",
    "    print(\"\\nNumber of Unique Values: \",df[column].nunique())\n",
    "    \n",
    "    #Get Distribution of Column    \n",
    "    print(\"\\nDistribution of column:\\n\")\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_missing_with_group_mode(df, groupby, column):      \n",
    "    print(\"\\nNo. of missing values before filling with group mode:\",df[column].isnull().sum())\n",
    "    \n",
    "    # Fill with local mode\n",
    "    mode_per_group = df.groupby(groupby)[column].transform(lambda x: x.mode().iat[0])\n",
    "    df[column] = df[column].fillna(mode_per_group)\n",
    "    \n",
    "    print(\"\\nNo. of missing values after filling with group mode:\",df[column].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Method to clean categorical field\n",
    "\n",
    "def clean_categorical_field(df,groupby,column,replace_value=None):\n",
    "    print(\"\\n-----------------------------------------------------\")\n",
    "    print(\"\\nCleaning steps \")\n",
    "    \n",
    "    #Replace with np.nan\n",
    "    if replace_value!=None:\n",
    "        df[column] = df[column].replace(replace_value,np.nan)\n",
    "        print(f\"\\nGarbage value {replace_value} is replaced with np.nan\")\n",
    "\n",
    "    #For each Customer_ID, assign same value for the column\n",
    "    fill_missing_with_group_mode(df,groupby,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle Outliers and null values\n",
    "def fix_inconsistent_values(df, groupby, column):      \n",
    "    print(\"\\nExisting Min, Max Values:\", df[column].apply([min, max]), sep='\\n', end='\\n')   \n",
    "    \n",
    "    df_dropped = df[df[column].notna()].groupby(groupby)[column].apply(list)\n",
    "    x, y = df_dropped.apply(lambda x: stats.mode(x)).apply([min, max])\n",
    "    print(\"x \", x)\n",
    "    print(\"y \", y)\n",
    "    mini, maxi = x[0], y[0]\n",
    "\n",
    "    # assign Wrong Values to NaN\n",
    "    col = df[column].apply(lambda x: np.NaN if ((x<mini)|(x>maxi)|(x<0)) else x)\n",
    "\n",
    "    # fill with local mode\n",
    "    mode_by_group = df.groupby(groupby)[column].transform(lambda x: x.mode()[0] if not x.mode().empty else np.NaN)\n",
    "    df[column] = col.fillna(mode_by_group)\n",
    "    df[column].fillna(df[column].mean(),inplace=True)\n",
    "\n",
    "    print(\"\\nAfter Cleaning Min, Max Values:\", df[column].apply([min, max]), sep='\\n', end='\\n') \n",
    "    print(\"\\nNo. of Unique values after Cleaning:\",df[column].nunique())\n",
    "    print(\"\\nNo. of Null values after Cleaning:\",df[column].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Method to clean Numerical Field\n",
    "def clean_numerical_field(df,groupby,column,strip=None,datatype=None,replace_value=None):\n",
    "    print(\"\\n-----------------------------------------------------\")\n",
    "    print(\"\\nCleaning steps \")\n",
    "    \n",
    "    #Replace with np.nan\n",
    "    if replace_value!=None:\n",
    "        df[column] = df[column].replace(replace_value,np.nan)\n",
    "        print(f\"\\nGarbage value {replace_value} is replaced with np.nan\")\n",
    "        \n",
    "    # Remove trailing & leading special characters\n",
    "    if df[column].dtype == object and strip is not None:\n",
    "        df[column] = df[column].str.strip(strip)\n",
    "        print(f\"\\nTrailing & leading {strip} are removed\")\n",
    "\n",
    "    # Change datatype\n",
    "    if datatype is not None:\n",
    "        df[column] = df[column].astype(datatype)\n",
    "        print(f\"\\nDatatype of {column} is changed to {datatype}\")\n",
    "\n",
    "    fix_inconsistent_values(df, groupby, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_countplot(df,column,user_friendly_column_name,rotation=0):\n",
    "    print(\"\\n-----------------------------------------------------\")\n",
    "    print(f'\\n{user_friendly_column_name} Distribution')\n",
    "    palette = \"deep\" \n",
    "    sns.set_palette(palette)\n",
    "    \n",
    "    sns.countplot(data=df, x=column)\n",
    "\n",
    "    plt.xlabel(f'{user_friendly_column_name}')\n",
    "    plt.ylabel('Number of Records')\n",
    "    plt.title(f'{user_friendly_column_name} Distribution')\n",
    "    plt.xticks(rotation=rotation)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_displot(df,column,user_friendly_column_name,rotation=0,bins=20):\n",
    "    print(\"\\n-----------------------------------------------------\")\n",
    "    print(f'\\n{user_friendly_column_name} Distribution')\n",
    "    palette = \"deep\" \n",
    "    sns.set_palette(palette)\n",
    "    \n",
    "    sns.displot(data=df, x=column, kde=True, bins=bins)\n",
    "\n",
    "    plt.xlabel(f'{user_friendly_column_name}')\n",
    "    plt.ylabel('Number of Records')\n",
    "    plt.title(f'{user_friendly_column_name} Distribution')\n",
    "    plt.xticks(rotation=rotation)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_stacked_bar(df,column1,column2,rotation=0):\n",
    "    print(\"\\n-----------------------------------------------------\")\n",
    "    print(f'\\n{column1} & {column2} Distribution')\n",
    "    palette = \"deep\" \n",
    "    sns.set_palette(palette)\n",
    "\n",
    "    pd.crosstab(df[column1], df[column2]).plot(kind='bar', stacked=True)\n",
    "    \n",
    "    plt.xlabel(f'{column1}')\n",
    "    plt.ylabel('Number of Records')\n",
    "    plt.title(f'{column1} & {column2} Distribution')\n",
    "    plt.xticks(rotation=rotation)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing - Categorical Variables\n",
    "\n",
    "### Credit Score \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 3 different Credit Score - Standard, Good & Poor.\n",
    "2. Distribution of credit score - \n",
    "\n",
    "   a) Standard - 53%\n",
    "   \n",
    "   b) Poor - 29%\n",
    "   \n",
    "   c) Good - 17%\n",
    "   \n",
    "3. There are no null values for Credit Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Credit_Score'\n",
    "user_friendly_name = 'Credit Score'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span> ID </span> <a class=\"anchor\" id=\"id\"></a>\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 100000 distinct records and no null values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get Details\n",
    "get_column_details(df_train,'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer ID \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. We have record of 12500 unique customers. \n",
    "2. Same customer can have different credit score. It means that on the basis of other values customer credit score can change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get Details \n",
    "get_column_details(df_train,'Customer_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check if same customer can have different credit score\n",
    "df_train.groupby(['Customer_ID'])['Credit_Score'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. In the training dataset, we have credit score for each customer over the course of 8 months(from January to August).\n",
    "2. Converted Month column from object to datetime value so that it can be further use for model building. \n",
    "3. Distribution of Credit_Scrore across different months is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Month'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Plot Distrbution with Credit_Score\n",
    "plot_stacked_bar(df_train,column_name,'Credit_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert Month to datetime object\n",
    "df_train['Month'] = pd.to_datetime(df_train.Month, format='%B').dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 9985 null values. \n",
    "2. Cleaning Step - Assign same Name value to each Customer_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Name'\n",
    "group_by = 'Customer_ID'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_categorical_field(df_train,group_by,column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSN \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 12501 unique SSN values in training dataset. \n",
    "2. 5572 entries has random/garbage value as SSN value\n",
    "3. Steps to Clean SSN -\n",
    "\n",
    "    i. Replace garbage value with np.nan\n",
    "    \n",
    "    ii. Assign same SSN value for each customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'SSN'\n",
    "group_by = 'Customer_ID'\n",
    "garbage_value = '#F%$D@*&8'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_categorical_field(df_train,group_by,column_name,garbage_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation \n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 16 unique Occupation values. \n",
    "2. 7062 records are marked with garbage value.\n",
    "3. Steps to Clean Occupation -\n",
    "\n",
    "    i. Replace garbage value with np.nan\n",
    "    \n",
    "    ii. Assign same Occupation value for each customer ID\n",
    "4. Distribution of Credit_Scrore across different occupation is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Occupation'\n",
    "group_by = 'Customer_ID'\n",
    "garbage_value = '_______'\n",
    "user_friendly_name = 'Occupation'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_categorical_field(df_train,group_by,column_name,garbage_value)\n",
    "\n",
    "#Plot Distribution with Credit_Score\n",
    "plot_stacked_bar(df_train,column_name,'Credit_Score',rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Loan\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 6260 unique values present for Type of Loan and there are null values present.\n",
    "2. Mapped all null values to *Not Specificed* for Type of Loan column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get Details of Type of Loan column\n",
    "get_column_details(df_train,'Type_of_Loan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Handle Type of Loan null values\n",
    "df_train['Type_of_Loan'].replace([np.NaN], 'Not Specified', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Mix\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 3 types of Credit Mix - Standard, Good, Bad\n",
    "2. About 20k records of Credit Mix is marked as a garbage value (_).\n",
    "3. Steps to Clean Credit Mix Field -\n",
    "\n",
    "    i. Replace garbage value with np.nan\n",
    "    \n",
    "    ii. Assign same Credit Mix value for each customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Credit_Mix'\n",
    "group_by = 'Customer_ID'\n",
    "garbage_value = '_'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_categorical_field(df_train,group_by,column_name,garbage_value)\n",
    "\n",
    "#Plot Distribution with Credit_Score\n",
    "plot_stacked_bar(df_train,column_name,'Credit_Score',rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payment of Min Amount\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 3 unique values present - Yes, No & NM.\n",
    "2. No missing values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Payment_of_Min_Amount'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Plot Distribution with Credit_Score\n",
    "plot_stacked_bar(df_train,column_name,'Credit_Score',rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payment Behaviour\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 6 unique values of Payment Behaviour -\n",
    "\n",
    "    Low_spent_Small_value_payments      \n",
    "    High_spent_Medium_value_payments    \n",
    "    Low_spent_Medium_value_payments     \n",
    "    High_spent_Large_value_payments     \n",
    "    High_spent_Small_value_payments     \n",
    "    Low_spent_Large_value_payments  \n",
    "   \n",
    "2. Amount 27% of records are for Low_spent_Small_value_payments\n",
    "    \n",
    "3. For 7.6k records, Payment Behaviour is filled with garbage value\n",
    "\n",
    "4. Steps to Clean Payment Behaviour Field -\n",
    "\n",
    "    i. Replace garbage value with np.nan\n",
    "    \n",
    "    ii. Assign same Payment Behaviour value for each customer ID\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Payment_Behaviour'\n",
    "group_by = 'Customer_ID'\n",
    "garbage_value = '!@9#%8'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_categorical_field(df_train,group_by,column_name,garbage_value)\n",
    "\n",
    "#Plot Distribution with Credit_Score\n",
    "plot_stacked_bar(df_train,column_name,'Credit_Score',rotation=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing - Numerical Variables\n",
    "\n",
    "**Cleaning Steps**\n",
    "1. Remove Trailing & Leading speical characters.\n",
    "2. Convert datatype from object to int/float if required.\n",
    "3. Replace null values & outliers with mode value when group by Customer_ID\n",
    "\n",
    "### Age\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are 1788 unique values of Age and it is stored as an object. Having 1788 distinct values of Age mean that there is a lot of dirty data.\n",
    "2. After cleaning up Age value, 43 distinct Age remains. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Age'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Age'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype='int')\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Income\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. Annual Income has no null values. \n",
    "2. Most customers have a low Annual income. Distribution is right skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Annual_Income'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Annual Income'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype='float')\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Inhand Salary\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are null values present.\n",
    "2. No outliers were present for Monthly Income Salary.\n",
    "3. Most customers have a low monthly income. Distribution is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Monthly_Inhand_Salary'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Monthly Inhand Salary'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Bank Accounts\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are some outliers,negative values in Num Bank Accounts\n",
    "2. After cleaning, there are 11 possible value of this field - Num Bank Accounts ranging from 0 to 10.\n",
    "3. Majority of customers has no. of bank accounts between 3 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Num_Bank_Accounts'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Number of Bank Accounts'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Credit Cards\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There are outliers present in the field as there are 1179 unique values of number of credit card.\n",
    "2. After removing outliers, number of credit cards range from 0 to 11 with most of the customers having credit cards in the range of 3 to 7 with peak at 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Num_Credit_Card'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Number of Credit Card'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest Rate\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. There were outliers present, after cleaning them up, interest rate ranges from 1% to 34%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Interest_Rate'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Interest Rate'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay from Due Date\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. Delay from due date is concentrated between 0 to 30 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Delay_from_due_date'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Delay from Due Date'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Delayed Payment\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Num_of_Delayed_Payment'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Number of Delayed Payment'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype='float')\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed Credit Limit\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Changed_Credit_Limit'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Changed Credit Limit'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype='float',replace_value='_')\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Credit Inquiries\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Num_Credit_Inquiries'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Number of Credit Inquiries'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_countplot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outstanding Debt\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Outstanding_Debt'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Outstanding Debt'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype=float)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Utilization Ratio\n",
    "\n",
    "**Summary**\n",
    "1. No cleaning is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Credit_Utilization_Ratio'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Credit Utilization Ratio'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit History Age\n",
    "\n",
    "**Summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['Credit_History_Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Month_Converter(val):\n",
    "    if pd.notnull(val):\n",
    "        years = int(val.split(' ')[0])\n",
    "        month = int(val.split(' ')[3])\n",
    "        return (years*12)+month\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "df_train['Credit_History_Age'] = df_train['Credit_History_Age'].apply(lambda x: Month_Converter(x)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Credit_History_Age'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Credit History Age'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,datatype=float)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total EMI per month\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Total_EMI_per_month'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Total EMI per month'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount Invested Monthly\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Amount_invested_monthly'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Amount invested monthly'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,datatype=float,strip='_')\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Balance\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Monthly_Balance'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Monthly Balance'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "df_train[column_name].replace('',np.nan)\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype=float,replace_value='__-333333333333333333333333333__')\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Loan\n",
    "\n",
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_name = 'Num_of_Loan'\n",
    "group_by = 'Customer_ID'\n",
    "user_friendly_name = 'Number of Loan'\n",
    "\n",
    "#Get Details\n",
    "get_column_details(df_train,column_name)\n",
    "\n",
    "#Cleaning\n",
    "clean_numerical_field(df_train,group_by,column_name,strip='_',datatype=float)\n",
    "\n",
    "#Plot Graph\n",
    "plot_displot(df_train,column_name,user_friendly_name,bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check if null values are present\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Drop columns\n",
    "print(\"Size of Dataset before dropping columns : \",df_train.shape)\n",
    "drop_columns = ['ID','Customer_ID','Name','SSN']\n",
    "df_train.drop(drop_columns,axis=1,inplace=True)\n",
    "print(\"Size of Dataset after dropping columns : \",df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.to_csv('cleaned_train.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scale_mapper = {\"Poor\":0, \"Standard\":1, \"Good\":2}\n",
    "df[\"Credit_Score\"] = df[\"Credit_Score\"].replace(scale_mapper)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = df.drop(columns=[\"Credit_Score\"]).select_dtypes(include=[\"number\"]).columns\n",
    "cat = df.drop(columns=[\"Credit_Score\"]).select_dtypes(include=[\"object\"]).columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"Credit_Score\")\n",
    "y = df[\"Credit_Score\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1. Applying Label Encoding for Target Variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "y = label_encoder.fit_transform(y) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2. Scaling Numerical Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "X[num] = scaler.fit_transform(X[num]) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3. Applying One Hot Encoding for Categorical Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,OrdinalEncoder\n",
    "\n",
    "OneHot_Encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\n",
    "OneHot_Encoder.fit(X[cat])\n",
    "one_hot_encoded = OneHot_Encoder.fit_transform(df[cat])\n",
    "one_hot_X = pd.DataFrame(one_hot_encoded, columns=OneHot_Encoder.get_feature_names_out(cat))\n",
    "df_encoded = pd.concat([X, one_hot_X], axis=1)\n",
    "X = df_encoded.drop(cat, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4. Oversampling Minority Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.15, random_state=42,stratify=y_res)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_res.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Neural Network Model Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "  keras.layers.Dense(200,activation = 'relu'),\n",
    "  keras.layers.Dropout(0.6),\n",
    "  keras.layers.Dense(100,activation = 'relu'),\n",
    "  keras.layers.Dropout(0.8),   \n",
    "  keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(filepath='Credit_Score_Classification_model.keras', monitor='val_loss', save_best_only=True)]\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=50,\n",
    "    batch_size = 32,\n",
    "    validation_data=(X_test,y_test),\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "from matplotlib.pyplot import figure\n",
    "epochs_range = range(9)\n",
    "\n",
    "figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(epochs_range, accuracy, label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('learning curve')\n",
    "plt.savefig('learning curve')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "__, ax = plt.subplots()\n",
    "print(classification_report(y_test,y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred), annot=True,fmt=\"g\", ax=ax)\n",
    "confusion_matrix(y_test,y_pred)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix') \n",
    "ax.xaxis.set_ticklabels(['Poor', 'Standard','Good'])\n",
    "ax.yaxis.set_ticklabels(['Poor', 'Standard','Good'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2289007,
     "sourceId": 3846912,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30474,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
